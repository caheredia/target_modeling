{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from src import find_empty_columns,  find_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/raw/targeting_model_data.csv' \n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of columns: {}'.format(data.shape[1]))\n",
    "print('Number of rows: {}'.format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at data \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition of target data, this demonstrates imbalanced data. And thus accuracy alone is not a good metric for assessing performance of model. \n",
    "data['FLOZVPMFT4626A'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and drop dulicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicates = find_duplicates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of duplicate columns dropped: {}'.format(len(duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data frame excluding dropped columns \n",
    "df = data.drop(columns=duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns that have > 80% missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_empty_columns(df, threshold=0.80):\n",
    "    \"\"\"A list of columns that have more > 80% missing values. \n",
    "\n",
    "    For each column computes the number of missing values. \n",
    "    If the value is greater than 80%, relative to column length,\n",
    "    then column name is added to list.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "    threshold : int (optional)\n",
    "        threshold for deleting columns \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    empty_columns : list\n",
    "        a list of columns with mostly empty values. \n",
    "    \"\"\"\n",
    "    empty_columns = []\n",
    "    for column in df:\n",
    "        if df[column].isna().sum() / len(df) > threshold:\n",
    "            empty_columns.append(column)\n",
    "    return empty_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_columns = find_empty_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of mostly empty columns dropped: {}'.format(len(empty_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame excluding dropped columns \n",
    "df = df.drop(columns=empty_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For data type int64 or float64 drop columns with low variance\n",
    "motivated by the Variance Threshold function https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_low_variance(data, threshold=0.18):\n",
    "     \"\"\"Finds columns with low variance.\n",
    "\n",
    "    Takes a dataframe as input. Creates a list of columns with low threshold.\n",
    "    These columns can then be dropped from original dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    data : pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "    threshold : int (optional)\n",
    "        threshold for deleting columns\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "    low_variance_columns : list\n",
    "        a list of columns with low variance.\n",
    "    \"\"\"\n",
    "    low_variance_columns = []\n",
    "    for column in data.columns: \n",
    "        if (data[column].dtype == 'float64') or (data[column].dtype == 'int64'): \n",
    "            if data[column].var() < threshold:\n",
    "                low_variance_columns.append(column)\n",
    "    return low_variance_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data frame excluding dropped columns \n",
    "low_var_columns = find_low_variance(df)\n",
    "df.drop(columns=low_var_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of low variance columns dropped: {}'.format(len(low_var_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Categorical values with low variance by converting to labels to dummy variables and summing the standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_low_var_categories(data, threshold=0.18):\n",
    "     \"\"\"Finds categorical columns with low variance.\n",
    "\n",
    "    Takes a dataframe as input. Creates a list of columns with low threshold.\n",
    "    These columns can then be dropped from original dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    data : pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "    threshold : int (optional)\n",
    "        threshold for deleting columns\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "    low_variance_columns : list\n",
    "        a list of columns with low variance.\n",
    "    \"\"\"\n",
    "    low_variance_columns = []\n",
    "    for column in data.columns: \n",
    "        if data[column].dtype == 'O': \n",
    "            if pd.get_dummies(df[column]).var().sum() < threshold:\n",
    "                low_variance_columns.append(column)\n",
    "    return low_variance_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_v_categories = find_low_var_categories(df)\n",
    "df.drop(columns=low_v_categories, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of low variance category columns dropped: {}'.format(len(low_v_categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace NaN values in numerical columns with median value of series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_NaN(data):\n",
    "    \"\"\"Replace NaN values with the median from numerical column. \n",
    "\n",
    "    Takes a dataframe as input. Iterates through the dataframe for numeric types columns.   \n",
    "    Replaces any NaN values with median value of column.  \n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    data : pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "    data : pandas.DataFrame\n",
    "        dataframe with columns containing replaced NaNs. \n",
    "    \"\"\"\n",
    "\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype != 'O':\n",
    "            data[column].fillna(data[column].median(), inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= replace_NaN(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find date like columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_like(df):\n",
    "    \"\"\"Finds time like columns.\n",
    "\n",
    "    Takes a dataframe as input. Iterates through the dataframe columns. \n",
    "    Returns a list of potential time like columns. \n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    df: pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "    low_variance_columns : list\n",
    "        a list of columns with low variance.\n",
    "    \"\"\"\n",
    "    time_columns = []\n",
    "    for column in df:\n",
    "        if df[column].dtype != 'O':\n",
    "            if df[column].mean() / 1000 > 1:\n",
    "                time_columns.append(column)\n",
    "    return time_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_columns = time_like(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[time_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visually inspecting items that are not potential time stamps\n",
    "not_time_columns = ['ibe8588DPLHE7435F', 'ibe8840PMLTL7040B']\n",
    "real_time_columns = [item for item in time_columns if item not in not_time_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_timestamp(df, time_columns):\n",
    "    \"\"\"Coverts columns in a dataframe to datetime.\n",
    "\n",
    "    Takes a dataframe and list of columns as input. \n",
    "    Converts columns in list to datetime format, depending on the length of value. \n",
    "    For values longer than 6, only the year is stripped. \n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    df: pandas.DataFrame\n",
    "        dataframe with columns to search\n",
    "    time_columns: list \n",
    "        list of time like columns\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "    df: pandas.DataFrame\n",
    "        dataframe with converted time columns \n",
    "    \"\"\"\n",
    "    for column in df[time_columns]:\n",
    "        # convert year\n",
    "        if len(str(int(df[column][0]))) == 4:\n",
    "            # convert to time stamp and then to int\n",
    "            df[column] = pd.to_datetime(df[column].astype(\n",
    "                int), format='%Y').astype(np.int64)\n",
    "\n",
    "        elif len(str(int(df[column][0]))) == 5:\n",
    "            # convert to time stamp and then to int\n",
    "            df[column] = pd.to_datetime(df[column].astype(\n",
    "                int), format='%Y%m').astype(np.int64)\n",
    "\n",
    "        # strip the year\n",
    "        elif len(str(int(df[column][0]))) == 6:\n",
    "            # truncate to year month and convert to time stamp, then to int\n",
    "            df[column] = df[column].astype(\n",
    "                int).astype(str).apply(lambda x: x[:4])\n",
    "            df[column] = pd.to_datetime(\n",
    "                df[column], format='%Y').astype(np.int64)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_to_timestamp(df, real_time_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ real_time_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned up dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/interim/cleaned_columns.csv'\n",
    "df.to_csv(path_or_buf=filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cleaned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/interim/cleaned_columns.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "X =pd.get_dummies(df.iloc[:,1:10]).values\n",
    "target = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, target, random_state=2)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)  # fit only on training data\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)  # apply same transformation to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify preprocess is working correctly \n",
    "print('Mean of random array: {}'.format(round(Xtrain[:,8].mean())))\n",
    "print('Std of random array: {}'.format(round(Xtrain[:,8].std())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# label data\n",
    "target_names = ['0', '1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/sgd.html#classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"huber\", max_iter=20)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"huber\", max_iter=20, class_weight='balanced')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)\n",
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=11)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)\n",
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier( n_estimators=100, n_jobs=-1, class_weight='balanced')\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guasian Niave Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# train the model\n",
    "clf = GaussianNB()\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
